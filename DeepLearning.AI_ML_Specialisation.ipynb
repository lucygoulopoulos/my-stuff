{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f6d6d4-0b72-4894-9ea6-e364684ff2b5",
   "metadata": {},
   "source": [
    "## **DeepLearning.AI - Machine Learning Specialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c398be-6f35-48b6-8498-3654e36b4a46",
   "metadata": {},
   "source": [
    "### **Linear regression:**\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X_train)\n",
    "\n",
    "sgdr = SGDRegressor(max_iter=1000)\n",
    "sgdr.fit(X_norm, y_train)\n",
    "\n",
    "b_norm = sgdr.intercept_\n",
    "w_norm = sgdr.coef_\n",
    "\n",
    "y_pred_sgd = sgdr.predict(X_norm)\n",
    "y_pred = np.dot(X_norm, w_norm) + b_norm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8dca5f-ab06-4fec-b8c6-e9625463fa7a",
   "metadata": {},
   "source": [
    "### **Neural networks:**\n",
    "\n",
    "- Activation = neuron = single logistic regression model (or other model)\n",
    "- Layer: a grouping of neurons which takes as input the same or similar features, and that in turn outputs a few numbers together\n",
    "- Each layer takes in a vector, does a calculation based on logistic regression (or other activation model), and outputs a vector\n",
    "- Main Python packages: TensorFlow or PyTorch\n",
    "- Fitting the model is faster if you normalise the data first\n",
    "- At the output layer, use the common sense activation function for the problem\n",
    "- On the hidden layers, always use ReLU (rectified linear unit) (basically looks like a call option)\n",
    "```\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy   # Loss function for classification model\n",
    "from tensorflow.keras.losses import MeanSquaredError     # Loss function for regression model\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Normalise data:\n",
    "X = np.array([[x1, x2, x3]])                             # X must be a row array of features\n",
    "norm_l = tf.keras.layers.Normalization(axis=-1)\n",
    "norm_l.adapt(X)\n",
    "Xn = norm_l(X)\n",
    "\n",
    "# Define the model\n",
    "tf.random.set_seed(1234)\n",
    "model = Sequential([\n",
    "                    Dense(units, activation),\n",
    "                    Dense(units, activation)            # Note: final layer needs units = 1\n",
    "                    ])\n",
    "\n",
    "# Fit the model\n",
    "model.compile(loss=BinaryCrossentropy(), optimizer=Adam(learning_rate=0.01))\n",
    "model.fit(Xn, y, epochs=10)\n",
    "# View results\n",
    "model.summary()\n",
    "W1, b1 = model.get_layer('layer1').get_weights()\n",
    "print(W1, b1)\n",
    "\n",
    "# Forward propagation\n",
    "X_testn = norm_l(X_test)\n",
    "predictions = model.predict(X_testn)\n",
    "yhat = (predictions >= 0.5).astype(int)\n",
    "```\n",
    "Parameters:\n",
    "- units: number of activations in the layer\n",
    "- activation: 'sigmoid', 'linear', 'relu'\n",
    "- loss: type of loss function (classification or regression)\n",
    "- optimizer: Adam (gradient descent with dynamic learning rate)\n",
    "- epochs: no. steps of gradient descent\n",
    "\n",
    "**Multiclass classification:**\n",
    "- Softmax regression: a generalisation of logistic regression\n",
    "- In final layer, use `activation='linear'`\n",
    "- Use loss function: `SparseCategoricalCrossentropy(from_logits=True)`\n",
    "- Then need to convert output to probabilities:\n",
    "```\n",
    "preds = model.predict(X_train)\n",
    "probs = tf.nn.softmax(preds).numpy()\n",
    "\n",
    "classification = []\n",
    "for i in range(len(preds)):\n",
    "    classification.append(np.argmax(preds[i]))\n",
    "```\n",
    "Two possibilities for loss function:\n",
    "- SparseCategorialCrossentropy: expects the target to be an integer corresponding to the index. For example, if there are 10 potential target values, y would be between 0 and 9.\n",
    "- CategoricalCrossEntropy: Expects the target value of an example to be one-hot encoded where the value at the target index is 1 while the other N-1 entries are zero. An example with 10 potential target values, where the target is 2 would be [0,0,1,0,0,0,0,0,0,0].\n",
    "\n",
    "**Convolutional layers:**\n",
    "- Alternative to Dense layers\n",
    "- Dense layers use all activation values from the previous layer\n",
    "- Convolutional layers are limited to only a subset of the previous activation values\n",
    "- Speeds up computation, needs less training data, and less prone to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfadb0e-6131-4e4e-a394-bed02f4306c3",
   "metadata": {},
   "source": [
    "### **Decision trees:**\n",
    "\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e853c624-25fd-402a-997b-e65b29d87519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
