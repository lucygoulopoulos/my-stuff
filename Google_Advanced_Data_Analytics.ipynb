{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f30757-e774-44b5-bb15-130634b84658",
   "metadata": {},
   "source": [
    "## **Google Advanced Data Analytics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "239ffee1-4c3b-4f8e-8bcd-c39f39cf374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operational packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy import stats\n",
    "import pickle\n",
    "\n",
    "# Visualisation packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, PredefinedSplit\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb488f50-3dfb-47e1-9249-51bb9728fc6a",
   "metadata": {},
   "source": [
    "### **Data validation:**\n",
    "```\n",
    "df.describe(include='all')\n",
    "df.info()\n",
    "df['column'].value_counts()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772f7fd8-1afb-4f1d-8262-92a2eaf59548",
   "metadata": {},
   "source": [
    "### **Confidence interval:**\n",
    "```\n",
    "stats.norm.interval(alpha=0.95, loc=sample_mean, scale=sample_std_error)\n",
    "sample_std_error = sample_sd / np.sqrt(n)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca3b0d9-f81b-4533-b285-dd4ee309e94c",
   "metadata": {},
   "source": [
    "### **Hypothesis testing:**\n",
    "\n",
    "Assuming normally distributed populations<br>\n",
    "If population SD is known, use z-test. If unknown, use t-test (fatter tails)<br>\n",
    "Note: if we are looking at proportions, must use z-test<br>\n",
    "\n",
    "**2 sample t-test (A/B testing):**<br>\n",
    "```\n",
    "p_value = stats.ttest_ind(a, b, equal_var, alternative)[1]\n",
    "equal_var: {True, False}\n",
    "alternative : {'two-sided', 'less', 'greater'}\n",
    "```\n",
    "\n",
    "**1 sample t-test:**<br>\n",
    "```\n",
    "p_value = stats.ttest_1samp(a, popmean, alternative)[1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa31697-6116-4381-8cf5-19b82e65c9fc",
   "metadata": {},
   "source": [
    "### **Linear regression:**\n",
    "\n",
    "To model a continuous variable (estimate mean of y)\n",
    "\n",
    "Key assumptions:\n",
    "- Linearity\n",
    "- Normal distribution of residuals\n",
    "- Independent observations\n",
    "- Homoscedasticity\n",
    "- For multiple regression: no multicollinearity between independent variables (low VIF)\n",
    "\n",
    "**Check scatter plots of pairs:**\n",
    "```\n",
    "sns.pairplot(df)\n",
    "```\n",
    "\n",
    "**Run regression:**\n",
    "```\n",
    "ols_formula = 'y_col ~ x1_col + x2_col + C(x_categorical_col)'\n",
    "OLS = ols(formula=ols_formula, data=df)\n",
    "model = OLS.fit()\n",
    "model.summary()\n",
    "fitted_values = model.fittedvalues\n",
    "residuals = model.resid\n",
    "sns.regplot(x=x_col, y=y_col, data=df)\n",
    "```\n",
    "\n",
    "**Check homoscedasticity (random cloud of fitted vs residuals):**\n",
    "```\n",
    "sns.scatterplot(fitted_values, residuals);\n",
    "```\n",
    "\n",
    "**Check normal distribution of residuals:**\n",
    "```\n",
    "sns.histplot(residuals);\n",
    "sm.qqplot(residuals, line='s');\n",
    "```\n",
    "\n",
    "**Check VIF (lower is better, minimum is 1):**\n",
    "```\n",
    "vif = [variance_inflation_factor(df_X.values, i) for i in range(df_X.shape[1])]\n",
    "df_vif = pd.DataFrame(vif, index=df_X.columns, columns=['VIF'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50e4b8f-9480-4500-a933-c32150f6cc10",
   "metadata": {},
   "source": [
    "### **Analysis of Variance:**\n",
    "\n",
    "**ANOVA:**<br>\n",
    "- Test the difference of means between >=3 groups<br>\n",
    "- Extension of t-tests<br>\n",
    "- Applied to results of linear regression model<br>\n",
    "- One-way ANOVA: compares the means of 1 continuous dependent variable in >=3 groups of 1 categorical variable. (H0: all means are equal)<br>\n",
    "- Two-way ANOVA: compares the means of 1 continuous dependent variable in >=3 groups of 2 categorical variables. (H0: both categories + their interaction all have no impact on mean)<br>\n",
    "- If PR(>F) is small, reject the null hypothesis\n",
    "```\n",
    "model = ols(formula=ols_formula, data=df).fit()\n",
    "sm.stats.anova_lm(model, typ=2)\n",
    "```\n",
    "ANOVA post-hoc test (Tukey's HSD test):<br>\n",
    "Pairwise comparison between all available groups while controlling for the error rate<br>\n",
    "Trying to isolate which group is different\n",
    "```\n",
    "tukey_oneway = pairwise_tukeyhsd(endog=df_y, groups=df_X, alpha=0.05)\n",
    "tukey_oneway.summary()\n",
    "```\n",
    "\n",
    "**ANCOVA:**<br>\n",
    "Test the difference of means between >=3 groups, while controlling for the effects of covariates (aka variables irrelevant to your test)<br>\n",
    "\n",
    "**MANOVA:**<br>\n",
    "Multi-variate analysis of variance<br>\n",
    "Compare how >=2 continuous outcome variables vary according to categorical independent variables<br>\n",
    "One-way: 1 categorical independent variable<br>\n",
    "Two-way: 2 categorical independent variables<br>\n",
    "H0: means of both continuous outcome variables are not impacted by the categorical independent variables\n",
    "\n",
    "**MANCOVA:**<br>\n",
    "MANOVA while controlling for the effects of covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934695db-e9e3-4953-87fb-cc4a9c284cba",
   "metadata": {},
   "source": [
    "### **Bias vs Variance:**\n",
    "\n",
    "High bias = simple / underfit<br>\n",
    "High variance = complex / overfit<br>\n",
    "\n",
    "**Regularised regression**\n",
    "Shrinks coefficients towards 0\n",
    "- Lasso: irrelevant coefficients are set to 0\n",
    "- Ridge: irrelevant coefficients shrink towards 0, but don't reach 0\n",
    "- Elastic net: combination of Lasso and Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87eae07-f283-4bdd-aac6-c4bc5701f161",
   "metadata": {},
   "source": [
    "### **Logistic regression:**\n",
    "\n",
    "To model a categorical dependent variable Y<br>\n",
    "Binomial logistic regression: Y is binary<br>\n",
    "\n",
    "Logit is the most common link function used to linearly relate the X variables to the probability of Y.\n",
    "\n",
    "Key assumptions:\n",
    "- Linearity: Linear relationship between each X variable and logit of P(Y=1)\n",
    "- Independent observations (can multiply probabilities)\n",
    "- No multicollinearity between independent variables\n",
    "- No extreme outliers\n",
    "\n",
    "logit(p) = log(p / (1-p)) = B0 + B1X1 + ... + BnXn -> Use MLE to find the betas\n",
    "```\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "print(clf.coef_, clf.intercept_)\n",
    "y_pred = clf.predict(X_test)\n",
    "clf.predict_proba(X_test)[::,-1]\n",
    "sns.regplot(x=x_col, y=y_col, data=df, logistic=True)\n",
    "```\n",
    "**Confusion matrix**\n",
    "```\n",
    "cm = metrics.confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "```\n",
    "**Precision, recall and accuracy**\n",
    "\n",
    "- Precision: Proportion of positive predictions that were true positives\n",
    "- Recall: Proportion of positives that the model was able to identify correctly\n",
    "- Accuracy: Proportion of data points that were correctly categorised\n",
    "- F1 (harmonic mean): 2 * precision * recall / (precision + recall)\n",
    "```\n",
    "print(metrics.precision_score(y_test, y_pred))\n",
    "print(metrics.recall_score(y_test, y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(metrics.f1_score(y_test, y_pred))\n",
    "```\n",
    "ROC curve and AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862dabde-72db-4f77-aac8-55c0b50d104c",
   "metadata": {},
   "source": [
    "### **Data preparation:**\n",
    "\n",
    "**Train-test split:**\n",
    "```\n",
    "df_X = df[[col_X1, col_X2, col_X3]]\n",
    "df_y = df[[col_y]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, stratify=df_y, test_size=0.3)\n",
    "```\n",
    "**Get dummies:**\n",
    "```\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "```\n",
    "**Scaling data:**\n",
    "```\n",
    "scaler = MinMaxScaler()   # or StandardScaler() or Normalizer()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "```\n",
    "**To export/import a fitted model:**\n",
    "```\n",
    "with open(path + 'filename.pickle', 'wb') as to_write:\n",
    "    pickle.dump(model, to_write)\n",
    "\n",
    "with open(path + 'filename.pickle', 'rb') as to_read:\n",
    "    model = pickle.load(to_read)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce574e64-2092-4822-b683-a3c73554c9b2",
   "metadata": {},
   "source": [
    "### **Naive Bayes:**\n",
    "\n",
    "Supervised | Classification\n",
    "- Assumption of independence among predictors\n",
    "- Does not require data scaling\n",
    "\n",
    "```\n",
    "from sklearn.naive_bayes import GaussianNB      # for continuous, normally distributed features\n",
    "from sklearn.naive_bayes import MultinomialNB   # for discrete features\n",
    "from sklearn.naive_bayes import BernoulliNB     # for Boolean features\n",
    "from sklearn.naive_bayes import CategoricalNB   # for categorical features\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cb3672-8caf-4d4d-bc00-f5be656d5434",
   "metadata": {},
   "source": [
    "### **K-Means:**\n",
    "\n",
    "Unsupervised | Classification (Partitioning)\n",
    "\n",
    "- Works better with scaled data\n",
    "\n",
    "4 steps:\n",
    "1. Initiate k centroids (choose k)\n",
    "2. Assign each data point to its nearest centroid\n",
    "3. Recalculate the centroid of each cluster\n",
    "4. Repeat step 2 and 3 until convergence\n",
    "```\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=k).fit(df)\n",
    "print(kmeans.labels_.shape)\n",
    "print(kmeans.labels_)\n",
    "print(np.unique(kmeans.labels_))\n",
    "print(kmeans.cluster_centers_)\n",
    "print(kms.inertia_)\n",
    "print(silhouette_score(df_X, kms.labels_)\n",
    "```\n",
    "**Inertia**: sum of squared distances between each observation and its nearest centroid<br>\n",
    "Elbow method: plot k values vs inertia, and choose highest k with a meaningful drop in inertia\n",
    "\n",
    "**Silhouette score**: mean of silhouette coefficients for all observations. Accounts for intra- and inter-cluster<br>\n",
    "-1 <= s <= 1<br>\n",
    "Want k to maximise silhouette score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec6c215-05f7-4861-81b2-197f166cbeb8",
   "metadata": {},
   "source": [
    "### **Decision Trees:**\n",
    "\n",
    "Supervised | Classification\n",
    "\n",
    "- Require no assumptions regarding the distribution of underlying data\n",
    "- Can handle collinearity\n",
    "- Little preprocessing required\n",
    "- Susceptibile to overfitting\n",
    "\n",
    "Each split is determined by which variables and cut-off values offer the most predictive power\n",
    "```\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, y_train)\n",
    "dt_pred = decision_tree.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(15,12))\n",
    "plot_tree(decision_tree, max_depth=2, fontsize=14, feature_names=df_X.columns, filled=True);\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "**Gini impurity:** how pure the node is, from 0 to 0.5.<br>\n",
    "0: the node is a leaf<br>\n",
    "0.5: all classes are equally represented in the node<br>\n",
    "\n",
    "**Hyperparameters:**\n",
    "- max_depth\n",
    "- min_samples_split\n",
    "- min_samples_leaf\n",
    "\n",
    "Tuning hyperparameters with gridsearch:\n",
    "```\n",
    "tuned_decision_tree = DecisionTreeClassifier()\n",
    "tree_para = {'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50],\n",
    "             'min_samples_leaf': [2, 5, 10, 20, 50]}\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1'}\n",
    "clf = GridSearchCV(tuned_decision_tree, tree_para, scoring=scoring, cv=5, refit='f1')\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_estimator_)\n",
    "print(clf.best_score_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a5a90-338d-4cbb-a472-c9a0c2c56f31",
   "metadata": {},
   "source": [
    "### **Random Forest:**\n",
    "\n",
    "Supervised | Classification or Regression | Ensemble\n",
    "\n",
    "**Bagging:**\n",
    "- AKA Bootstrap Aggregating\n",
    "- Bootstrapping = sampling with replacement\n",
    "- Each individual model is called a base learner\n",
    "- Each base learner is trained on a unique random subset of the training data\n",
    "\n",
    "**Random Forest:**\n",
    "- Bagging based on decision trees\n",
    "- Each tree uses a random subset of the available features (so not all features)\n",
    "- Reduces variance, but not bias\n",
    "\n",
    "**Hyperparameters:**\n",
    "- All the same as decision trees, plus:\n",
    "- max_features\n",
    "- n_estimators\n",
    "- max_samples\n",
    "```\n",
    "from sklearn.ensemble import RandomForestClassifier   # for classification tasks\n",
    "from sklearn.ensemble import RandomForestRegressor    # for regression tasks\n",
    "rf = RandomForestClassifier()\n",
    "rf_cv = GridSearchCV(rf, cv_params, scoring=scoring, cv=5, refit='f1')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f3b742-3a9f-419c-b114-d4a62bdb2053",
   "metadata": {},
   "source": [
    "### **Boosting:**\n",
    "\n",
    "Supervised | Classification or Regression | Ensemble\n",
    "\n",
    "- Builds an ensemble of weak learners sequentially\n",
    "- Each consecutive base learner tries to correct the errors of the one before\n",
    "\n",
    "**AdaBoost:**\n",
    "- AKA Adaptive Boosting\n",
    "- Tree-based boosting methodology\n",
    "- Each consecutive base learner assigns greater weight to the observations incorrectly predicted by the preceding learner\n",
    "- Final prediction is based on weighted vote (classification) or weighted mean prediction (regression)\n",
    "- Runs slowly, because the base learners can't be trained in parallel\n",
    "\n",
    "Advantages:\n",
    "- Reduces variance and bias\n",
    "- Easy to interpret\n",
    "- Doesn't require data to be scaled or normalised\n",
    "- Can handle multicollinearity\n",
    "- Can use numeric or categorical inputs\n",
    "- Robust to outliers\n",
    "\n",
    "**Gradient Boosting (GBMs):**\n",
    "- Each base learner in the sequence is built to predict the residual errors of the model that preceded it\n",
    "- Tree-based\n",
    "```\n",
    "from xgboost import XGBClassifier   # For classification tasks\n",
    "from xgboost import XGBRegressor   # For regression tasks\n",
    "from xgboost import plot_importance\n",
    "xgb = XGBClassifier(objective='binary:logistic')\n",
    "xgb_cv = GridSearchCV(xgb, cv_params, scoring=scoring, cv=5, refit='f1')\n",
    "xgb_cv.fit(X_train, y_train)\n",
    "```\n",
    "Advantages:\n",
    "- High accuracy\n",
    "- Scalable\n",
    "- Work well with missing data\n",
    "- Usual advantages of tree-based methods\n",
    "\n",
    "Disadvantages:\n",
    "- Black-box model\n",
    "- Lots of hyperparameters\n",
    "- Prone to overfitting\n",
    "- Doesn't extrapolate well to new data\n",
    "\n",
    "Hyperparameters:\n",
    "- max_depth\n",
    "- n_estimators\n",
    "- learning_rate\n",
    "- min_child_weight\n",
    "- colsample_bytree\n",
    "- subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f140cce-9dbe-4220-b8e9-1c9787f9edd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
